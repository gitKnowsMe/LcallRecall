From 9413a603a41c827e55628c45c8621b1bf2848d2e Mon Sep 17 00:00:00 2001
From: gitKnowsMe <103797430+gitKnowsMe@users.noreply.github.com>
Date: Tue, 16 Sep 2025 12:18:00 -0400
Subject: [PATCH] fix: Restore real AI functionality by upgrading
 llama-cpp-python
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

- Fix RAG and Direct LLM returning mock/no responses after production merge
- Upgrade llama-cpp-python from 0.2.19 to 0.3.16 for Phi-2 support
- Remove all mock mode fallbacks per user requirements
- Fix API parameter mismatch (prompt -> q) in Direct LLM endpoint
- Restore proper error handling in model loading

Resolves: "This used to work perfect before the merged branches"
Tested: âœ… Phi-2 model loads with Apple M1 Pro Metal acceleration
Tested: âœ… Real AI responses in both RAG and Direct LLM modes

ðŸ¤– Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>
---
 backend/app/api/llm.py              |  6 +-
 backend/app/services/llm_service.py | 93 ++++++++++-------------------
 2 files changed, 34 insertions(+), 65 deletions(-)

diff --git a/backend/app/api/llm.py b/backend/app/api/llm.py
index fa3be3e..ab8ae9c 100644
--- a/backend/app/api/llm.py
+++ b/backend/app/api/llm.py
@@ -88,7 +88,7 @@ async def direct_llm_chat(
 
 @router.get("/stream")
 async def stream_llm_get(
-    prompt: str,
+    q: str,
     max_tokens: int = 1024,
     temperature: float = 0.7,
     token: Optional[str] = None,
@@ -102,11 +102,11 @@ async def stream_llm_get(
     """
     try:
         # Validate prompt
-        prompt = str(prompt) if prompt is not None else ""
+        prompt = str(q) if q is not None else ""
         if not prompt or not prompt.strip():
             raise HTTPException(
                 status_code=400,
-                detail="Prompt parameter is required and cannot be empty"
+                detail="Query parameter 'q' is required and cannot be empty"
             )
         
         # Handle authentication for EventSource (token in query params)
diff --git a/backend/app/services/llm_service.py b/backend/app/services/llm_service.py
index d79df51..000a651 100644
--- a/backend/app/services/llm_service.py
+++ b/backend/app/services/llm_service.py
@@ -25,72 +25,50 @@ class ModelManager:
         if self._model is not None:
             logger.info("Model already loaded")
             return True
-            
-        try:
-            if not os.path.exists(self._model_path):
-                logger.warning(f"Model not found at {self._model_path}")
-                logger.info("Using mock mode for development")
-                self._model = "mock"  # Use mock mode
-                return True
-            
-            logger.info(f"Loading Phi-2 model from {self._model_path}")
-            
-            # Initialize thread pool for blocking operations
-            self._executor = ThreadPoolExecutor(max_workers=1)
-            
-            try:
-                # Load model in thread pool to avoid blocking
-                loop = asyncio.get_event_loop()
-                self._model = await loop.run_in_executor(
-                    self._executor,
-                    self._load_model
-                )
-                logger.info("âœ… Phi-2 model loaded successfully")
-                return True
-            except Exception as model_error:
-                logger.warning(f"Failed to load model: {model_error}")
-                logger.info("Falling back to mock mode for development")
-                self._model = "mock"  # Use mock mode as fallback
-                return True
-            
-        except Exception as e:
-            logger.error(f"Failed to initialize model service: {e}")
-            logger.info("Using mock mode for development")
-            self._model = "mock"  # Use mock mode
-            return True
+
+        if not os.path.exists(self._model_path):
+            raise RuntimeError(f"Model not found at {self._model_path}")
+
+        logger.info(f"Loading Phi-2 model from {self._model_path}")
+
+        # Initialize thread pool for blocking operations
+        self._executor = ThreadPoolExecutor(max_workers=1)
+
+        # Load model in thread pool to avoid blocking
+        loop = asyncio.get_event_loop()
+        self._model = await loop.run_in_executor(
+            self._executor,
+            self._load_model
+        )
+        logger.info("âœ… Phi-2 model loaded successfully")
+        return True
     
     def _load_model(self) -> Llama:
         """Load the model (runs in thread pool)"""
-        return Llama(
-            model_path=self._model_path,
-            n_ctx=2048,  # Smaller context window to reduce memory usage
-            n_batch=256,  # Smaller batch size
-            n_threads=2,  # Fewer threads
-            verbose=False,
-            seed=42,  # Reproducible outputs
-            use_mlock=False,  # Don't lock memory
-            use_mmap=True,  # Use memory mapping
-            n_gpu_layers=0  # Force CPU only
-        )
+        try:
+            return Llama(
+                model_path=self._model_path,
+                n_ctx=2048,     # More conservative context window
+                n_batch=256,    # More conservative batch size
+                n_threads=2,    # Fewer threads to reduce resource contention
+                verbose=True,   # Enable verbose to see detailed error messages
+                seed=42,
+                use_mlock=False,  # Don't lock memory
+                use_mmap=True     # Use memory mapping for efficiency
+            )
+        except Exception as e:
+            logger.error(f"Detailed model loading error: {e}")
+            raise e
     
     def is_loaded(self) -> bool:
         """Check if model is loaded"""
         return self._model is not None
     
-    def _is_mock_mode(self) -> bool:
-        """Check if running in mock mode"""
-        return self._model == "mock"
-    
     async def generate(self, prompt: str, max_tokens: int = 1024, temperature: float = 0.7) -> str:
         """Generate response (non-streaming)"""
         if not self.is_loaded():
             raise RuntimeError("Model not loaded")
         
-        # Mock mode for development
-        if self._is_mock_mode():
-            await asyncio.sleep(0.5)  # Simulate processing time
-            return f"Mock response to: {prompt[:50]}... This is a simulated AI response for development purposes."
-        
         try:
             loop = asyncio.get_event_loop()
             response = await loop.run_in_executor(
@@ -112,15 +90,6 @@ class ModelManager:
         if not self.is_loaded():
             raise RuntimeError("Model not loaded")
         
-        # Mock mode for development  
-        if self._is_mock_mode():
-            mock_response = f"Mock streaming response to: {prompt[:50]}... This is a simulated AI response for development purposes with streaming simulation."
-            words = mock_response.split()
-            for word in words:
-                await asyncio.sleep(0.1)  # Simulate streaming delay
-                yield word + " "
-            return
-        
         try:
             # Create a queue for streaming tokens
             token_queue = asyncio.Queue()
-- 
2.39.5 (Apple Git-154)

